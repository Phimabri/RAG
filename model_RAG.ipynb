{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/mamba/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.12/site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/mamba/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/mamba/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/mamba/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/mamba/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/mamba/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/mamba/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/mamba/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/mamba/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/mamba/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/mamba/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/mamba/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/mamba/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/mamba/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/mamba/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/mamba/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/mamba/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/mamba/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /opt/mamba/lib/python3.12/site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.12/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/mamba/lib/python3.12/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/mamba/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/mamba/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/mamba/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/mamba/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/mamba/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/mamba/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/mamba/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.12/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: pip in /opt/mamba/lib/python3.12/site-packages (24.0)\n",
      "Channels:\n",
      " - pytorch\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: sentence_transformers in /opt/mamba/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/mamba/lib/python3.12/site-packages (from sentence_transformers) (4.40.2)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.12/site-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/mamba/lib/python3.12/site-packages (from sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/mamba/lib/python3.12/site-packages (from sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/mamba/lib/python3.12/site-packages (from sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/mamba/lib/python3.12/site-packages (from sentence_transformers) (1.13.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/mamba/lib/python3.12/site-packages (from sentence_transformers) (0.23.0)\n",
      "Requirement already satisfied: Pillow in /opt/mamba/lib/python3.12/site-packages (from sentence_transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/mamba/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/mamba/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/mamba/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/mamba/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/mamba/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.127)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/mamba/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/mamba/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/mamba/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/mamba/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/mamba/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: sacremoses in /opt/mamba/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /opt/mamba/lib/python3.12/site-packages (from sacremoses) (2024.5.15)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.12/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.12/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.12/site-packages (from sacremoses) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install --upgrade pip\n",
    "!conda install -y -c pytorch faiss-cpu\n",
    "!pip install sentence_transformers\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, FlaubertModel, pipeline\n",
    "import faiss\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_llm = 'microsoft/Phi-3-mini-128k-instruct'\n",
    "model_name_embedding = \"dangvantuan/sentence-camembert-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeddings = '/home/onyxia/work/StatApp-RAG/camemBERT/embeddings_doc2.csv'\n",
    "path_text_and_embeddings = '/home/onyxia/work/StatApp-RAG/camemBERT/text_and_embeddings_doc2.csv'\n",
    "path_index = '/home/onyxia/work/StatApp-RAG/camemBERT/faiss_index_doc2'\n",
    "path_QA = '/home/onyxia/work/StatApp-RAG/Q&A/output_triplets_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db20533d5ea743ab93505b9a091a18af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Cela ouvrira une invite dans votre notebook pour entrer votre token d'accès\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/mamba/lib/python3.12/site-packages (0.2.0)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-5.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece --index-url=https://pypi.org/simple\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type camembert to instantiate a model of type flaubert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of FlaubertModel were not initialized from the model checkpoint at dangvantuan/sentence-camembert-large and are newly initialized: ['attentions.0.k_lin.bias', 'attentions.0.k_lin.weight', 'attentions.0.out_lin.bias', 'attentions.0.out_lin.weight', 'attentions.0.q_lin.bias', 'attentions.0.q_lin.weight', 'attentions.0.v_lin.bias', 'attentions.0.v_lin.weight', 'attentions.1.k_lin.bias', 'attentions.1.k_lin.weight', 'attentions.1.out_lin.bias', 'attentions.1.out_lin.weight', 'attentions.1.q_lin.bias', 'attentions.1.q_lin.weight', 'attentions.1.v_lin.bias', 'attentions.1.v_lin.weight', 'attentions.10.k_lin.bias', 'attentions.10.k_lin.weight', 'attentions.10.out_lin.bias', 'attentions.10.out_lin.weight', 'attentions.10.q_lin.bias', 'attentions.10.q_lin.weight', 'attentions.10.v_lin.bias', 'attentions.10.v_lin.weight', 'attentions.11.k_lin.bias', 'attentions.11.k_lin.weight', 'attentions.11.out_lin.bias', 'attentions.11.out_lin.weight', 'attentions.11.q_lin.bias', 'attentions.11.q_lin.weight', 'attentions.11.v_lin.bias', 'attentions.11.v_lin.weight', 'attentions.12.k_lin.bias', 'attentions.12.k_lin.weight', 'attentions.12.out_lin.bias', 'attentions.12.out_lin.weight', 'attentions.12.q_lin.bias', 'attentions.12.q_lin.weight', 'attentions.12.v_lin.bias', 'attentions.12.v_lin.weight', 'attentions.13.k_lin.bias', 'attentions.13.k_lin.weight', 'attentions.13.out_lin.bias', 'attentions.13.out_lin.weight', 'attentions.13.q_lin.bias', 'attentions.13.q_lin.weight', 'attentions.13.v_lin.bias', 'attentions.13.v_lin.weight', 'attentions.14.k_lin.bias', 'attentions.14.k_lin.weight', 'attentions.14.out_lin.bias', 'attentions.14.out_lin.weight', 'attentions.14.q_lin.bias', 'attentions.14.q_lin.weight', 'attentions.14.v_lin.bias', 'attentions.14.v_lin.weight', 'attentions.15.k_lin.bias', 'attentions.15.k_lin.weight', 'attentions.15.out_lin.bias', 'attentions.15.out_lin.weight', 'attentions.15.q_lin.bias', 'attentions.15.q_lin.weight', 'attentions.15.v_lin.bias', 'attentions.15.v_lin.weight', 'attentions.16.k_lin.bias', 'attentions.16.k_lin.weight', 'attentions.16.out_lin.bias', 'attentions.16.out_lin.weight', 'attentions.16.q_lin.bias', 'attentions.16.q_lin.weight', 'attentions.16.v_lin.bias', 'attentions.16.v_lin.weight', 'attentions.17.k_lin.bias', 'attentions.17.k_lin.weight', 'attentions.17.out_lin.bias', 'attentions.17.out_lin.weight', 'attentions.17.q_lin.bias', 'attentions.17.q_lin.weight', 'attentions.17.v_lin.bias', 'attentions.17.v_lin.weight', 'attentions.18.k_lin.bias', 'attentions.18.k_lin.weight', 'attentions.18.out_lin.bias', 'attentions.18.out_lin.weight', 'attentions.18.q_lin.bias', 'attentions.18.q_lin.weight', 'attentions.18.v_lin.bias', 'attentions.18.v_lin.weight', 'attentions.19.k_lin.bias', 'attentions.19.k_lin.weight', 'attentions.19.out_lin.bias', 'attentions.19.out_lin.weight', 'attentions.19.q_lin.bias', 'attentions.19.q_lin.weight', 'attentions.19.v_lin.bias', 'attentions.19.v_lin.weight', 'attentions.2.k_lin.bias', 'attentions.2.k_lin.weight', 'attentions.2.out_lin.bias', 'attentions.2.out_lin.weight', 'attentions.2.q_lin.bias', 'attentions.2.q_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.v_lin.weight', 'attentions.20.k_lin.bias', 'attentions.20.k_lin.weight', 'attentions.20.out_lin.bias', 'attentions.20.out_lin.weight', 'attentions.20.q_lin.bias', 'attentions.20.q_lin.weight', 'attentions.20.v_lin.bias', 'attentions.20.v_lin.weight', 'attentions.21.k_lin.bias', 'attentions.21.k_lin.weight', 'attentions.21.out_lin.bias', 'attentions.21.out_lin.weight', 'attentions.21.q_lin.bias', 'attentions.21.q_lin.weight', 'attentions.21.v_lin.bias', 'attentions.21.v_lin.weight', 'attentions.22.k_lin.bias', 'attentions.22.k_lin.weight', 'attentions.22.out_lin.bias', 'attentions.22.out_lin.weight', 'attentions.22.q_lin.bias', 'attentions.22.q_lin.weight', 'attentions.22.v_lin.bias', 'attentions.22.v_lin.weight', 'attentions.23.k_lin.bias', 'attentions.23.k_lin.weight', 'attentions.23.out_lin.bias', 'attentions.23.out_lin.weight', 'attentions.23.q_lin.bias', 'attentions.23.q_lin.weight', 'attentions.23.v_lin.bias', 'attentions.23.v_lin.weight', 'attentions.3.k_lin.bias', 'attentions.3.k_lin.weight', 'attentions.3.out_lin.bias', 'attentions.3.out_lin.weight', 'attentions.3.q_lin.bias', 'attentions.3.q_lin.weight', 'attentions.3.v_lin.bias', 'attentions.3.v_lin.weight', 'attentions.4.k_lin.bias', 'attentions.4.k_lin.weight', 'attentions.4.out_lin.bias', 'attentions.4.out_lin.weight', 'attentions.4.q_lin.bias', 'attentions.4.q_lin.weight', 'attentions.4.v_lin.bias', 'attentions.4.v_lin.weight', 'attentions.5.k_lin.bias', 'attentions.5.k_lin.weight', 'attentions.5.out_lin.bias', 'attentions.5.out_lin.weight', 'attentions.5.q_lin.bias', 'attentions.5.q_lin.weight', 'attentions.5.v_lin.bias', 'attentions.5.v_lin.weight', 'attentions.6.k_lin.bias', 'attentions.6.k_lin.weight', 'attentions.6.out_lin.bias', 'attentions.6.out_lin.weight', 'attentions.6.q_lin.bias', 'attentions.6.q_lin.weight', 'attentions.6.v_lin.bias', 'attentions.6.v_lin.weight', 'attentions.7.k_lin.bias', 'attentions.7.k_lin.weight', 'attentions.7.out_lin.bias', 'attentions.7.out_lin.weight', 'attentions.7.q_lin.bias', 'attentions.7.q_lin.weight', 'attentions.7.v_lin.bias', 'attentions.7.v_lin.weight', 'attentions.8.k_lin.bias', 'attentions.8.k_lin.weight', 'attentions.8.out_lin.bias', 'attentions.8.out_lin.weight', 'attentions.8.q_lin.bias', 'attentions.8.q_lin.weight', 'attentions.8.v_lin.bias', 'attentions.8.v_lin.weight', 'attentions.9.k_lin.bias', 'attentions.9.k_lin.weight', 'attentions.9.out_lin.bias', 'attentions.9.out_lin.weight', 'attentions.9.q_lin.bias', 'attentions.9.q_lin.weight', 'attentions.9.v_lin.bias', 'attentions.9.v_lin.weight', 'embeddings.weight', 'ffns.0.lin1.bias', 'ffns.0.lin1.weight', 'ffns.0.lin2.bias', 'ffns.0.lin2.weight', 'ffns.1.lin1.bias', 'ffns.1.lin1.weight', 'ffns.1.lin2.bias', 'ffns.1.lin2.weight', 'ffns.10.lin1.bias', 'ffns.10.lin1.weight', 'ffns.10.lin2.bias', 'ffns.10.lin2.weight', 'ffns.11.lin1.bias', 'ffns.11.lin1.weight', 'ffns.11.lin2.bias', 'ffns.11.lin2.weight', 'ffns.12.lin1.bias', 'ffns.12.lin1.weight', 'ffns.12.lin2.bias', 'ffns.12.lin2.weight', 'ffns.13.lin1.bias', 'ffns.13.lin1.weight', 'ffns.13.lin2.bias', 'ffns.13.lin2.weight', 'ffns.14.lin1.bias', 'ffns.14.lin1.weight', 'ffns.14.lin2.bias', 'ffns.14.lin2.weight', 'ffns.15.lin1.bias', 'ffns.15.lin1.weight', 'ffns.15.lin2.bias', 'ffns.15.lin2.weight', 'ffns.16.lin1.bias', 'ffns.16.lin1.weight', 'ffns.16.lin2.bias', 'ffns.16.lin2.weight', 'ffns.17.lin1.bias', 'ffns.17.lin1.weight', 'ffns.17.lin2.bias', 'ffns.17.lin2.weight', 'ffns.18.lin1.bias', 'ffns.18.lin1.weight', 'ffns.18.lin2.bias', 'ffns.18.lin2.weight', 'ffns.19.lin1.bias', 'ffns.19.lin1.weight', 'ffns.19.lin2.bias', 'ffns.19.lin2.weight', 'ffns.2.lin1.bias', 'ffns.2.lin1.weight', 'ffns.2.lin2.bias', 'ffns.2.lin2.weight', 'ffns.20.lin1.bias', 'ffns.20.lin1.weight', 'ffns.20.lin2.bias', 'ffns.20.lin2.weight', 'ffns.21.lin1.bias', 'ffns.21.lin1.weight', 'ffns.21.lin2.bias', 'ffns.21.lin2.weight', 'ffns.22.lin1.bias', 'ffns.22.lin1.weight', 'ffns.22.lin2.bias', 'ffns.22.lin2.weight', 'ffns.23.lin1.bias', 'ffns.23.lin1.weight', 'ffns.23.lin2.bias', 'ffns.23.lin2.weight', 'ffns.3.lin1.bias', 'ffns.3.lin1.weight', 'ffns.3.lin2.bias', 'ffns.3.lin2.weight', 'ffns.4.lin1.bias', 'ffns.4.lin1.weight', 'ffns.4.lin2.bias', 'ffns.4.lin2.weight', 'ffns.5.lin1.bias', 'ffns.5.lin1.weight', 'ffns.5.lin2.bias', 'ffns.5.lin2.weight', 'ffns.6.lin1.bias', 'ffns.6.lin1.weight', 'ffns.6.lin2.bias', 'ffns.6.lin2.weight', 'ffns.7.lin1.bias', 'ffns.7.lin1.weight', 'ffns.7.lin2.bias', 'ffns.7.lin2.weight', 'ffns.8.lin1.bias', 'ffns.8.lin1.weight', 'ffns.8.lin2.bias', 'ffns.8.lin2.weight', 'ffns.9.lin1.bias', 'ffns.9.lin1.weight', 'ffns.9.lin2.bias', 'ffns.9.lin2.weight', 'layer_norm1.0.bias', 'layer_norm1.0.weight', 'layer_norm1.1.bias', 'layer_norm1.1.weight', 'layer_norm1.10.bias', 'layer_norm1.10.weight', 'layer_norm1.11.bias', 'layer_norm1.11.weight', 'layer_norm1.12.bias', 'layer_norm1.12.weight', 'layer_norm1.13.bias', 'layer_norm1.13.weight', 'layer_norm1.14.bias', 'layer_norm1.14.weight', 'layer_norm1.15.bias', 'layer_norm1.15.weight', 'layer_norm1.16.bias', 'layer_norm1.16.weight', 'layer_norm1.17.bias', 'layer_norm1.17.weight', 'layer_norm1.18.bias', 'layer_norm1.18.weight', 'layer_norm1.19.bias', 'layer_norm1.19.weight', 'layer_norm1.2.bias', 'layer_norm1.2.weight', 'layer_norm1.20.bias', 'layer_norm1.20.weight', 'layer_norm1.21.bias', 'layer_norm1.21.weight', 'layer_norm1.22.bias', 'layer_norm1.22.weight', 'layer_norm1.23.bias', 'layer_norm1.23.weight', 'layer_norm1.3.bias', 'layer_norm1.3.weight', 'layer_norm1.4.bias', 'layer_norm1.4.weight', 'layer_norm1.5.bias', 'layer_norm1.5.weight', 'layer_norm1.6.bias', 'layer_norm1.6.weight', 'layer_norm1.7.bias', 'layer_norm1.7.weight', 'layer_norm1.8.bias', 'layer_norm1.8.weight', 'layer_norm1.9.bias', 'layer_norm1.9.weight', 'layer_norm2.0.bias', 'layer_norm2.0.weight', 'layer_norm2.1.bias', 'layer_norm2.1.weight', 'layer_norm2.10.bias', 'layer_norm2.10.weight', 'layer_norm2.11.bias', 'layer_norm2.11.weight', 'layer_norm2.12.bias', 'layer_norm2.12.weight', 'layer_norm2.13.bias', 'layer_norm2.13.weight', 'layer_norm2.14.bias', 'layer_norm2.14.weight', 'layer_norm2.15.bias', 'layer_norm2.15.weight', 'layer_norm2.16.bias', 'layer_norm2.16.weight', 'layer_norm2.17.bias', 'layer_norm2.17.weight', 'layer_norm2.18.bias', 'layer_norm2.18.weight', 'layer_norm2.19.bias', 'layer_norm2.19.weight', 'layer_norm2.2.bias', 'layer_norm2.2.weight', 'layer_norm2.20.bias', 'layer_norm2.20.weight', 'layer_norm2.21.bias', 'layer_norm2.21.weight', 'layer_norm2.22.bias', 'layer_norm2.22.weight', 'layer_norm2.23.bias', 'layer_norm2.23.weight', 'layer_norm2.3.bias', 'layer_norm2.3.weight', 'layer_norm2.4.bias', 'layer_norm2.4.weight', 'layer_norm2.5.bias', 'layer_norm2.5.weight', 'layer_norm2.6.bias', 'layer_norm2.6.weight', 'layer_norm2.7.bias', 'layer_norm2.7.weight', 'layer_norm2.8.bias', 'layer_norm2.8.weight', 'layer_norm2.9.bias', 'layer_norm2.9.weight', 'layer_norm_emb.bias', 'layer_norm_emb.weight', 'position_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_embedding = FlaubertModel.from_pretrained(model_name_embedding)\n",
    "tokenizer_embedding = AutoTokenizer.from_pretrained(model_name_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c3a762d3604692a2167b34109480f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(model_name_llm, device_map=\"cuda\", torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, )\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(model_name_llm)\n",
    "\n",
    "model_llm = model_llm.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du pipeline de génération de texte\n",
    "generator_llm = pipeline('text-generation', model=model_llm, tokenizer=tokenizer_llm, max_length=3000)\n",
    "\n",
    "generation_args = {\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "def generate_response(question, context):\n",
    "    # Construction de la requête en utilisant le contexte et la question fournis\n",
    "    prompt = f\"[INST] Tu es un assistant respectueux et utile, réponds toujours de manière précise, assertive et polie en quelques mots en français conversationnel. \" \\\n",
    "             f\"Contexte : {context} \" \\\n",
    "             f\"Réponds précisément à la question ci-dessous en 300 caractères avec l'aide du contexte et ne pose pas de question, et réponds en français : \" \\\n",
    "             f\"{question} [/INST]\"\n",
    "\n",
    "    # Génération de la réponse en utilisant le modèle\n",
    "    responses = generator_llm(prompt, num_return_sequences=1, **generation_args)\n",
    "    response = responses[0]['generated_text'] if responses else None\n",
    "    response = response.replace(prompt, \"\").strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_csv(path_embeddings).drop('Unnamed: 0', axis=1)\n",
    "QA = pd.read_csv(path_QA)\n",
    "text_and_embeddings = pd.read_csv(path_text_and_embeddings)\n",
    "index = faiss.read_index(path_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Quels sont les fonds propres de l'entreprise?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les fonds propres de Sogécap ne sont pas spécifiés dans le contexte fourni.\n",
      "La réponse est basée sur le context trouvé dans le document 2: ['Au sein des risques de marché, Sogécap est sensible aux chocs sur le niveau des taux d’intérêt, des marchés actions et des spreads de crédit. En lien avec l’activité d’assurance vie épargne, le risque de rachats, au sein des risques de souscription vie, est également significatif.Les événements majeurs des derniers exercices, la crise sani-taire puis la guerre en Ukraine qui se poursuit, ont entraîné des répercussions macro-économiques importantes et ont renforcé le niveau d’incertitude. Leurs répercussions sur Sogécap ont principalement porté sur le volume d’activité, mais la société n’a pas connu d’évolution majeure de son profil de risque.', 'Au sein des risques de marché, Sogécap est sensible aux chocs sur le niveau des taux d’intérêt, des marchés actions et des spreads de crédit. En lien avec l’activité d’assurance vie épargne, le risque de rachats, au sein des risques de souscription vie, est également significatif.Les événements majeurs des derniers exercices, la crise sani-taire puis la guerre en Ukraine qui se poursuit, ont entraîné des répercussions macro-économiques importantes et ont renforcé le niveau d’incertitude. Leurs répercussions sur Sogécap ont principalement porté sur le volume d’activité, mais la société n’a pas connu d’évolution majeure de son profil de risque.Valorisation du bilan économiqueLes principes généraux de valorisation des actifs et passifs retenus sont ceux prévus par les dispositions de la Directive Solvabilité\\xa02, des Règlements Délégués 2015/35 et 2019/981 et des notices de l’ACPR.', 'MorbiditéLe risque de morbidité reflète le risque de perte sur les contrats de prévoyance résultant d’une sous estimation et/ou de changements dans le niveau de la sinistralité en termes d’incapacité et d’invalidité.LongévitéLe risque de longévité correspond au risque engendré par une sous-estimation de la durée de vie moyenne des assurés du portefeuille engendrant le paiement de rentes sur une durée plus longue qu’évaluée initialement.PandémieLe risque de pandémie est le risque de perte sur les contrats d’assurance vie épargne et de prévoyance lié à une forte augmentation des taux de mortalité et de morbidité.DépensesLe risque de dépenses est le risque que les dépenses réelles de fonctionnement (personnel, commis-sions aux intermédiaires de vente, infrastructure informatique…) soient plus élevées que le niveau estimé initialement.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer_embedding.encode(question, add_special_tokens=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model_embedding(tokens).last_hidden_state\n",
    "question_embedding = torch.mean(outputs, dim=1)\n",
    "D, I = index.search(question_embedding, 3)\n",
    "context = [text_and_embeddings['text'].iloc[indice] for indice in I[0]]\n",
    "\n",
    "print(generate_response(question, \"\\n\".join(context)))\n",
    "print(\"La réponse est basée sur le context trouvé dans le document 2:\", context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
